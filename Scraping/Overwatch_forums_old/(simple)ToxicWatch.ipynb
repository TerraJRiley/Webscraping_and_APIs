{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: To scrape as much of the old Overwatch forums as I can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for use within the scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the getting of the HTML that I'm looking for.\n",
    "def get_html(url):\n",
    "    return BeautifulSoup(requests.get(url).content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to specifically spit out a list of tuples\n",
    "# tuple[0] is the url of the topic\n",
    "# tuple[1] is the number of pages in the topic\n",
    "def urls_with_numbers(forum_page_url):\n",
    "#current_forum_page = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "    forum_soup = get_html(forum_page_url)                                       # Input URL get Soup\n",
    "    topic_url_list = ['https://us.battle.net' + topic.attrs['href'] for topic in forum_soup.find_all(attrs={'class': \"ForumTopic\"})]\n",
    "    topic_tuples = []                                                           # Above is list of urls of topics in forum page\n",
    "    count = 0                                                                   # Instantiating empty list and starting count\n",
    "    for forum_topic in forum_soup.find_all(attrs={'class': \"ForumTopic\"}):      # for i in all th3e forum topic infos\n",
    "        posts_num = json.loads(forum_topic.attrs['data-forum-topic'])['lastPosition'] # Turning each info bit into a dict\n",
    "        topic_pages = posts_num//20 + 1                     # Number of posts in Topic / by num allowed per page +1 for 1st\n",
    "        if posts_num%20 != 0 and posts_num > 20:            # If there's a remainder page\n",
    "            topic_pages += 1                                # Add remainder page\n",
    "        topic_tuples.append((topic_url_list[count],topic_pages)) # Add the url to the page amount in tuples\n",
    "        count += 1                                          # Keep track of the count\n",
    "    return topic_tuples                                     # Returns list of tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_posts(list_of_dicts, saver, forum_page):\n",
    "    return pd.DataFrame(list_of_dicts, columns = ['text',\n",
    "                            'date',\n",
    "                            'ids_dict',\n",
    "                            'post_num',\n",
    "                            'auth_posts',\n",
    "                            'prof_link',\n",
    "                            'title',\n",
    "                            'forum_page',\n",
    "                            'statuses',\n",
    "                            'topic_url']).to_csv(saver + str(forum_page), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was used to simplify the most recent version of the scraping code.\n",
    "# It'll likely be replaced by the smaller function further down with hardcoded bits removed.\n",
    "words = []\n",
    "dates = []\n",
    "TopicPosts = []\n",
    "title = []\n",
    "forum_page = []\n",
    "prof_link = []\n",
    "auth_posts = []\n",
    "topic_keys  = []\n",
    "def turn_to_dict(post, words = words, \n",
    "                 dates = dates, TopicPosts = TopicPosts, \n",
    "                 title = title, forum_page = forum_page, \n",
    "                 prof_link = prof_link, auth_posts = auth_posts, topic_keys = topic_keys):\n",
    "    post_dict = {\"If you're seeing this\": \"something went wrong.\"}\n",
    "    try:\n",
    "        post_dict = {                               # Creation & Statement of dicts\n",
    "            'text'      : words[post].contents,                     # Text of the post\n",
    "            'date'      : dates[post].attrs['data-tooltip-content'],# Date of the (unedited) post\n",
    "            'ids_dict'  : TopicPosts[post].attrs['data-topic-post'],# Author info & votes of the post\n",
    "            'post_num'  : TopicPosts[post].attrs['id'] ,            # Post number in the topic\n",
    "            'title'     : title,                    # Title of Topic\n",
    "            'forum_page': forum_page,               # Page in the forum\n",
    "            'topic_url' : topic_keys[post][0]}\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        post_dict['statuses']   = TopicPosts[post].attrs['data-topic']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        post_dict['prof_link']  = prof_link[post].attrs['href'][0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        post_dict['auth_posts'] = auth_posts[post][0]  # Number of posts author has made\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return post_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace above function in usability\n",
    "def add_attempt(dictionary, title, addition):\n",
    "    try:\n",
    "        dictionary[str(title)] = addition\n",
    "    except:\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code simplification later.\n",
    "edt = ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for clicking next until the final page of the topic has been scraped.\n",
    "def click_next():\n",
    "    more_pages = True\n",
    "    try:\n",
    "        nexts = []\n",
    "        for element in browser.find_elements_by_class_name('Button-content'):\n",
    "            if element.text == 'NEXT':\n",
    "                nexts.append(element)\n",
    "        nexts[1].click()\n",
    "        #not_last_page = True\n",
    "    except:\n",
    "        more_pages = False\n",
    "    return more_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions of scraping code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the original code that I used to scrape the forum.  I wanted to get a small and simple amount of data and as such thie only obtains the first page of every topic and will only grab topics in pages divisible by 5 or 10, depending on what I set it to previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL SCRAPING SAMPLING CODE\n",
    "#ops = webdriver.chrome.options.Options()\n",
    "#ops.add_argument('--dns-prefetch-disable')\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path)#, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(1,1000):\n",
    "    try:\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        if forum_page % 100 == 0:\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):\n",
    "            try:\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                #print('start dict')\n",
    "                while more_pages == True:                                     # Click on Title i\n",
    "                    soup = BeautifulSoup(browser.page_source, 'lxml')         # Soup of all\n",
    "                    words = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'}) # Words\n",
    "                    dates = soup.find_all('a',{\"class\" :'TopicPost-timestamp'})     # Dates\n",
    "                    for post in range(len(words)):\n",
    "                        post_dict = {                                         # Creation & Statement of dicts\n",
    "                            'text' : words[post].text,\n",
    "                            'date' : dates[post].text\n",
    "                        }\n",
    "                        list_of_dicts.append(post_dict)\n",
    "         \n",
    "                    more_pages = click_next()                           # Clicks \"NEXT\" otherwise return False\n",
    "                    browser.get(current_list)                                            # Back\n",
    "                    title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "            except:\n",
    "                print(\"Woops\", forum_page, i)\n",
    "                save_posts(list_of_dicts, saver, forum_page)\n",
    "                sleep(10)\n",
    "                browser.refresh()\n",
    "                browser.get(current_list)\n",
    "    except:\n",
    "        print(\"Woops\", forum_page)\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "        sleep(30)\n",
    "        browser.refresh()\n",
    "        browser.get(current_list)\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an expanded version of the previous code, still with selenium.  I wanted to grab as much information from the forums as possible.\n",
    "I also added the ops to the webdriver because a Stack Overflow page suggested it for the Timeout error I kept getting when running this code.  It did not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE WITH PROBABLY TOO MANY BROWSER CALLS\n",
    "ops = webdriver.chrome.options.Options()\n",
    "ops.add_argument('--dns-prefetch-disable')\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Pearl_'                          # Path to save\n",
    "\n",
    "for forum_page in range(0,1001):     # BE CAREFUL WITH THIS             # For each page in chunk of posts\n",
    "    try:                                                                # Failsafe within forum page\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)                                       # Go to forum page\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")# Define list of topic pages to click\n",
    "        if forum_page % 100 == 0 & forum_page != 0:                                       # Failsafe saver per 100 pages\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):                                # For every title in the list\n",
    "            try:                                                        # Failsafe in topic page\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                #print('start dict')\n",
    "                #sleep(2)\n",
    "                while more_pages == True:                               # While true that there's more pages...\n",
    "                    title      = browser.find_element_by_class_name('Topic-title').text\n",
    "                    dates      = [e.get_attribute('data-tooltip-content') for e in browser.find_elements_by_class_name('TopicPost-timestamp') if e.text != ' (Edited)']\n",
    "                    words      = [e.text for e in browser.find_elements_by_class_name('TopicPost-bodyContent')]\n",
    "                    nums_dict  = [e.get_attribute('data-topic-post') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "                    #sleep(2)\n",
    "                    post_num   = [e.get_attribute('id') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "                    auth_posts = [e.text for e in browser.find_elements_by_class_name('Author-posts') if e.get_attribute('data-toggle') == 'tooltip']\n",
    "                    prof_link  = [e.get_attribute('href') for e in browser.find_elements_by_class_name('Author-avatar ')]\n",
    "                    all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "                    auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "                    #print('dict components defined')\n",
    "                    for post in range(len(words)):                      # For each post in topic\n",
    "                        try:\n",
    "                            post_dict = {                               # Creation & Statement of dicts\n",
    "                                'text'      : words[post],              # Text of the post\n",
    "                                'date'      : dates[post],              # Date of the (unedited) post\n",
    "                                'nums_dict' : nums_dict[post],          # Author info & votes of the post\n",
    "                                'post_num'  : post_num[post],           # Post umber in the topic\n",
    "                                'prof_link' : prof_link[post],          # Link to author profile\n",
    "                                'auth_img'  : auth_img[post],           # Profile image of author\n",
    "                                'title'     : title,                    # Title of Topic\n",
    "                                'forum_page': forum_page}               # Page in the forum\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            post_dict['auth_posts'] = auth_posts[post]  # Number of posts author has made\n",
    "                        except:\n",
    "                            pass\n",
    "                        list_of_dicts.append(post_dict)\n",
    "                    more_pages = click_next()                           # Clicks \"NEXT\" otherwise return False\n",
    "                    #print('next clicked')\n",
    "                browser.get(current_list)                               # Returns to current forum page\n",
    "                #sleep(3)                                              # Tiny safety sleep\n",
    "                #print('slept')\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "                \n",
    "            except:\n",
    "                print(\"Woops\", forum_page, i)                           # Informs of the problem\n",
    "                save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "                sleep(10)                                               # Safety sleep\n",
    "                browser.refresh()                                       # Safety Refresh\n",
    "                browser.get(current_list)                               # Return to forum page\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine for freshness\n",
    "    except:\n",
    "        print(\"Woops\", forum_page)                                      # Informs that there's a problem\n",
    "        save_posts(list_of_dicts, saver, forum_page)                    # Save current posts\n",
    "        sleep(30)                                                       # Long Safety Sleep\n",
    "        browser.refresh()                                               # Safety Refresh\n",
    "        browser.get(current_list)                                       # Return to forum page\n",
    "        \n",
    "save_posts(list_of_dicts, saver, forum_page)                            # Finally save the posts\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])             # Return the dataframe for looking! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunitly the above code resulted in a kind of timeout error so I decided to try a simpler approach.  This was when I decided to limit the number of requests that I made for the browser by instead using BeaufitulSoup once I got the code for the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ops = webdriver.chrome.options.Options()\n",
    "#ops.add_argument('--dns-prefetch-disable')\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path)#, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(1,1000):\n",
    "    try:\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        if forum_page % 100 == 0:\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):                                # For every title in the list\n",
    "            try:                                                        # Failsafe in topic page\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                next_page = 2\n",
    "                current_topic = browser.current_url\n",
    "                while more_pages == True:                               # While true that there's more pages...\n",
    "                    soup       = BeautifulSoup(browser.page_source, 'lxml')         # Soup of words\n",
    "                    title      = soup.find(attrs={'class':'Topic-title'}).contents\n",
    "                    TopicPosts = soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "                    dates      = [e for e in soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents != ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']]\n",
    "                    words      = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "                    auth_posts = [e.contents for e in soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "                    prof_link  = soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "                    all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "                    auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "                    for post in range(len(words)):                      # For each post in topic\n",
    "                        try:\n",
    "                            post_dict = {                               # Creation & Statement of dicts\n",
    "                                'text'      : words[post].contents,                     # Text of the post\n",
    "                                'date'      : dates[post].attrs['data-tooltip-content'],# Date of the (unedited) post\n",
    "                                'ids_dict'  : TopicPosts[post].attrs['data-topic-post'],# Author info & votes of the post\n",
    "                                'post_num'  : TopicPosts[post].attrs['id'] ,            # Post number in the topic\n",
    "                                'statuses'  : TopicPosts[post].attrs['data-topic'],\n",
    "                                'prof_link' : prof_link[post].attrs['href'],            # Link to author profile\n",
    "                                'auth_img'  : auth_img[post],           # Profile image of author\n",
    "                                'title'     : title,                    # Title of Topic\n",
    "                                'forum_page': forum_page}               # Page in the forum\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            post_dict['auth_posts'] = auth_posts[post]  # Number of posts author has made\n",
    "                        except:\n",
    "                            pass\n",
    "                        list_of_dicts.append(post_dict)\n",
    "                    try:\n",
    "                        browser.get(current_topic + '?page=' + str(next_page))\n",
    "                        soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "                        if soup.find(attrs = {'class':'error-type'}).contents[0] == '404':\n",
    "                            more_pages = False\n",
    "                        else:\n",
    "                            next_page += 1\n",
    "                    except:\n",
    "                        more_pages = False\n",
    "                browser.get(current_list)                               # Returns to current forum page\n",
    "                #sleep(3)                                                # Tiny safety sleep\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "                \n",
    "            except:\n",
    "                print(\"Woops\", forum_page, i)                           # Informs of the problem\n",
    "                save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "                sleep(10)                                               # Safety sleep\n",
    "                browser.refresh()                                       # Safety Refresh\n",
    "                browser.get(current_list)                               # Return to forum page\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine for freshness\n",
    "    except:\n",
    "        print(\"Woops\", forum_page)\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "        sleep(30)\n",
    "        browser.refresh()\n",
    "        browser.get(current_list)\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS is our new official one??\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path)#, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(72,1000):\n",
    "    try:\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        if forum_page % 100 == 0:\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):                                # For every title in the list\n",
    "            try:                                                        # Failsafe in topic page\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                next_page = 2\n",
    "                current_topic = browser.current_url\n",
    "                while more_pages == True:                               # While true that there's more pages...\n",
    "                    soup       = BeautifulSoup(browser.page_source, 'lxml')         # Soup of words\n",
    "                    title      = soup.find(attrs={'class':'Topic-title'}).contents\n",
    "                    TopicPosts = soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "                    dates      = [e for e in soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents != ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']]\n",
    "                    words      = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "                    auth_posts = [e.contents for e in soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "                    prof_link  = soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "                    all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "                    auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "                    for post in range(len(words)):                      # For each post in topic\n",
    "                        try:\n",
    "                            post_dict = {                               # Creation & Statement of dicts\n",
    "                            'text'      : words[post].contents,                     # Text of the post\n",
    "                            'date'      : dates[post].attrs['data-tooltip-content'],# Date of the (unedited) post\n",
    "                            'ids_dict'  : TopicPosts[post].attrs['data-topic-post'],# Author info & votes of the post\n",
    "                            'post_num'  : TopicPosts[post].attrs['id'] ,            # Post number in the topic\n",
    "                            'auth_img'  : auth_img[post],           # Profile image of author\n",
    "                            'title'     : title,                    # Title of Topic\n",
    "                            'forum_page': forum_page}               # Page in the forum\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            post_dict['statuses']   = TopicPosts[post].attrs['data-topic']\n",
    "                            post_dict['prof_link']  = prof_link[post].attrs['href']\n",
    "                            post_dict['auth_posts'] = auth_posts[post]  # Number of posts author has made\n",
    "                        except:\n",
    "                            pass\n",
    "                        list_of_dicts.append(post_dict)\n",
    "                    try:\n",
    "                        browser.get(current_topic + '?page=' + str(next_page))\n",
    "                        soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "                        #print('attempted new page')\n",
    "                        if soup.find(attrs = {'class':'error-type'}).contents[0] == '404':\n",
    "                            #print(\"got a 404\")\n",
    "                            more_pages = False\n",
    "                        #else:\n",
    "                        #    print(\"there is another page and we added 1\")\n",
    "                        #    next_page += 1\n",
    "                    except:\n",
    "                        next_page += 1\n",
    "                browser.get(current_list)                               # Returns to current forum page\n",
    "                #sleep(3)                                                # Tiny safety sleep\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Canceled!\")\n",
    "            except:\n",
    "                print(\"TimedOut on Forum Section:\", forum_page,\"Topic number:\", i, \"Topic Page:\", next_page-1)                           # Informs of the problem\n",
    "                save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "                sleep(10)                                               # Safety sleep\n",
    "                browser.refresh()                                       # Safety Refresh\n",
    "                browser.get(current_list)                               # Return to forum page\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine for freshness\n",
    "            #except:\n",
    "            #    print('SOMETHING ELSE WENT WRONG')\n",
    "            #    pass\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Canceled!\")\n",
    "    except:\n",
    "        print(\"TimedOut on Forum Section:\", forum_page)\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "        sleep(30)\n",
    "        browser.refresh()\n",
    "        browser.get(current_list)\n",
    "    #except:\n",
    "    #    print(\"SOMETHING ELSE WENT WRONG\")\n",
    "    #    pass\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had decided to attempt to find the bug by removing the try/excepts and no dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS is just before deleting a bunch of junk\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(0,100):\n",
    "    #try:\n",
    "    topic_keys = urls_with_numbers('https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page))\n",
    "    if forum_page % 100 == 0:\n",
    "        save_posts(list_of_dicts, saver, forum_page) \n",
    "    for topic_stats in topic_keys:                                # For every title in the list\n",
    "        for page in range(topic_stats[1]):\n",
    "            page += 1               # TRY \"for page +1 in range(topic_stats[1]):\" later\n",
    "            if page > 1:\n",
    "                URL = topic_stats[0] + '?page=' + str(page)\n",
    "            else:\n",
    "                URL = topic_stats[0]\n",
    "            page_soup  = get_html(URL)         # Soup of words\n",
    "            title      = page_soup.find(attrs={'class':'Topic-title'}).contents\n",
    "            dates      = [e for e in page_soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents != ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']]\n",
    "            words      = page_soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "            TopicPosts = page_soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "            auth_posts = [e.contents for e in page_soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "            prof_link  = page_soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "        for page in range(topic_info[1]):                      # For each post in topic\n",
    "\n",
    "            turn_to_dict(post)\n",
    "            list_of_dicts.append(post_dict)\n",
    "        try:\n",
    "            resoup = get_html(title_list[i] + '?page=' + str(next_page))\n",
    "            if resoup.find(attrs = {'class':'error-type'}).contents[0] == '404':\n",
    "                more_pages = False\n",
    "        except:\n",
    "            next_page += 1\n",
    "        resoup = get_html(current_list)                               # Returns to current forum page\n",
    "        #except KeyboardInterrupt:\n",
    "        #    print(\"Canceled!\")\n",
    "        #except:\n",
    "        #    print(\"TimedOut on Forum Section:\", forum_page,\"Topic number:\", i, \"Topic Page:\", next_page-1)                           # Informs of the problem\n",
    "        #    save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "        #    sleep(10)    \n",
    "        print(\"Success!:\", forum_page,\"Topic number:\", i, \"Topic Page:\", next_page-1)\n",
    "    #except KeyboardInterrupt:\n",
    "    #    print(\"Canceled!\")\n",
    "    #except:\n",
    "    #    print(\"TimedOut on Forum Section:\", forum_page)\n",
    "    #    save_posts(list_of_dicts, saver, forum_page)\n",
    "    #    sleep(30)\n",
    "    #    resoup = get_html(current_list)\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_chromedriver = '../Garage/chromedriver'                          # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)       # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_'                                 # Path to save\n",
    "\n",
    "for forum_page in range(0,1001):                                         # For each page in chunk of posts\n",
    "    current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "    browser.get(current_list)                                        # Go to forum page\n",
    "    title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")# Define list of topic pages to click\n",
    "    if forum_page % 100 == 0:                                        # Failsafe saver per 100 pages\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "    for i in range(len(title_list)):                                 # For every title in the list\n",
    "        title_list[i].click()                                    # Click on Title i\n",
    "        more_pages = True                                        # True means there's more pages\n",
    "        print('start dict')\n",
    "        while more_pages == True:                                # While true that there's more pages...\n",
    "            title      = browser.find_element_by_class_name('Topic-title').text\n",
    "            dates      = [e.get_attribute('data-tooltip-content') for e in browser.find_elements_by_class_name('TopicPost-timestamp') if e.text != ' (Edited)']\n",
    "            words      = [e.text for e in browser.find_elements_by_class_name('TopicPost-bodyContent')]\n",
    "            nums_dict  = [e.get_attribute('data-topic-post') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "            post_num   = [e.get_attribute('id') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "            auth_posts = [e.text for e in browser.find_elements_by_class_name('Author-posts') if e.get_attribute('data-toggle') == 'tooltip']\n",
    "            prof_link  = [e.get_attribute('href') for e in browser.find_elements_by_class_name('Author-avatar ')]\n",
    "            all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "            auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "            print('dict components defined')\n",
    "            for post in range(len(words)):                       # For each post in topic\n",
    "                try:\n",
    "                    post_dict = {                                    # Creation & Statement of dicts\n",
    "                        'text'      : words[post],                   # Text of the post\n",
    "                        'date'      : dates[post],                   # Date of the (unedited) post\n",
    "                        'nums_dict' : nums_dict[post],               # Author info & votes of the post\n",
    "                        'post_num'  : post_num[post],                # Post umber in the topic\n",
    "                        'prof_link' : prof_link[post],               # Link to author profile\n",
    "                        'auth_img'  : auth_img[post],                # Profile image of author\n",
    "                        'title'     : title,                         # Title of Topic\n",
    "                        'forum_page': forum_page                     # Page in the forum\n",
    "                    }\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    post_dict['auth_posts'] = auth_posts[post]   # Number of posts author has made\n",
    "                except:\n",
    "                    pass\n",
    "                list_of_dicts.append(post_dict)\n",
    "                print('dict created and appended')\n",
    "            more_pages = click_next()                            # Clicks \"NEXT\" otherwise return False\n",
    "            print('next clicked')\n",
    "        browser.get(current_list)                                # Returns to current forum page\n",
    "        sleep(0.5)                                               # Tiny safety sleep\n",
    "        print('slept')\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "save_posts(list_of_dicts, saver, forum_page)                             # Finally save the posts\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])              # Return the dataframe for looking! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the latest version of the scraping code.  Very buggy.  At this point I had to shelf the project to focus on other things, both because these edits didn't fix the timeout error and because the effort for my jupothixic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS is for turning into the Requests library\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_'                   # The path & starting name for saving\n",
    "\n",
    "for forum_page in range(101):                                   # Will go through all the forum pages specified\n",
    "    topic_keys = urls_with_numbers('https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page))\n",
    "    if forum_page % 100 == 0:\n",
    "        save_posts(list_of_dicts, saver, forum_page) \n",
    "    for topic_stats in topic_keys:                              # For every title in the list\n",
    "        for page in range(topic_stats[1]):                      # for page # in range of # of pages in the topic\n",
    "            #page += 1                                          # Add 1 to compensate for starting at 0\n",
    "            if page > 1:                                        # Basically making sure it's not the first page\n",
    "                URL = topic_stats[0] + '?page=' + str(page)     # This is basicslly for getting to all the pages\n",
    "            else:\n",
    "                URL = topic_stats[0]                            # This will be the first page\n",
    "            sleep(5)\n",
    "            \n",
    "            # This chunk defines the lists of things that we want per row\n",
    "            page_soup  = get_html(URL)                          # Soup of the page\n",
    "            title      = page_soup.find(attrs={'class':'Topic-title'}).contents\n",
    "            dates      = [e for e in page_soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents !=edt]\n",
    "            words      = page_soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "            TopicPosts = page_soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "            auth_posts = [e.contents for e in page_soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "            prof_link  = page_soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "            print(\"F-Page:\", forum_page, \"Posts/Topic:\", topic_stats[1],\"Page:\",page,\"URL:\",URL )\n",
    "            if page == 1:\n",
    "                print(\"Skipped duplicate page\")\n",
    "            else:\n",
    "                for post in range(len(words)):                  # For each post that's present\n",
    "                    list_of_dicts.append(turn_to_dict(post, words, \n",
    "                 dates, TopicPosts, title, forum_page, \n",
    "                 prof_link, auth_posts, topic_keys))            # Add the dict of post to list of posts\n",
    "        print(\"END F-page:\", forum_page,\"Topic Page:\", page, \"Last URL:\", URL) # Sanity check\n",
    "df = save_posts(list_of_dicts, saver, forum_page)               # Should both save the data & create a df to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = save_posts(list_of_dicts, saver, forum_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list_of_dicts, columns = ['text',\n",
    "                            'date',\n",
    "                            'ids_dict',\n",
    "                            'post_num',\n",
    "                            'auth_posts',\n",
    "                            'prof_link',\n",
    "                            'title',\n",
    "                            'forum_page',\n",
    "                            'statuses',\n",
    "                            'topic_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_topic_links = ['https://us.battle.net' + topic.attrs['href'] for topic in resoup.find_all(attrs={'class': \"ForumTopic\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = resoup.find_all(attrs={'class': \"ForumTopic\"})\n",
    "for i in range(len(test)):\n",
    "    print('-------')\n",
    "    print(i)\n",
    "    posts = json.loads(test[i].attrs['data-forum-topic'])['lastPosition']\n",
    "    print('posts:', posts)\n",
    "    pages = posts//20 + 1\n",
    "    print(\"pages:\", pages)\n",
    "    if posts%20 > 0 and posts > 21:\n",
    "        pages += 1\n",
    "        print(\"has remainder\", posts%20, 'pages:', pages)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = resoup.find_all(attrs={'class': \"ForumTopic\"})\n",
    "for i in range(len(test)):\n",
    "    posts = json.loads(test[i].attrs['data-forum-topic'])['lastPosition']\n",
    "    pages = posts//20 + 1\n",
    "    if posts%20 > 0 and posts > 21:\n",
    "        pages += 1\n",
    "for  in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_24to77     = pd.read_csv('./data/Overwatch_24to77_posts_77.csv')\n",
    "#df_149        = pd.read_csv('./data/Overwatch_posts_149')          # An error skipped pages 24 to 77\n",
    "df_s0to150    = pd.read_csv('./data/Overwatch_Sample_to1500_160')\n",
    "df_s150to1000 = pd.read_csv('./data/Overwatch_SAMPLE150to1000_990')\n",
    "df_s1500      = pd.read_csv('./data/Overwatch_SAMPLEposts_1500')\n",
    "df_s1500to5k  = pd.read_csv('./data/Overwatch_Sample1500to5k_5000')\n",
    "df_s5kto10k   = pd.read_csv('./data/Overwatch_Sample5kto10k_7250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s0to150['date']      = df_s0to150['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s150to1000['date']   = df_s150to1000['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s1500['date']        = df_s1500['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s1500to5k['date']    = df_s1500to5k['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s5kto10k['date']     = df_s5kto10k['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_listed = [    \n",
    "    df_s0to150,\n",
    "    df_s150to1000,\n",
    "    df_s1500,\n",
    "    df_s1500to5k,\n",
    "    df_s5kto10k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df_s150to1000['text']:\n",
    "    try:\n",
    "        if '<Removed by forum moderator for toxicity>' in text:\n",
    "            print(text)\n",
    "    except:\n",
    "        print(text)\n",
    "        #print(df_s0to150['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs_listed:\n",
    "    for text in df['text']:\n",
    "        try:\n",
    "            if '<Removed' in text:\n",
    "                df.head(1)\n",
    "                print(text)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs_listed:\n",
    "    print(df['date'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested Via Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates      = [e.get_attribute('data-tooltip-content') for e in browser.find_elements_by_class_name('TopicPost-timestamp') if e.text != ' (Edited)']\n",
    "words      = [e.text for e in browser.find_elements_by_class_name('TopicPost-bodyContent')]\n",
    "nums_dict  = [e.get_attribute('data-topic-post') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "post_num   = [e.get_attribute('id') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "auth_posts = [e.text for e in browser.find_elements_by_class_name('Author-posts') if e.get_attribute('data-toggle') == 'tooltip']\n",
    "prof_link  = [e.get_attribute('href') for e in browser.find_elements_by_class_name('Author-avatar ')]\n",
    "all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = driver.find_elements_by_xpath(\"xpath\")\n",
    "links = [link.get_attribute('href') for link in driver.find_elements_by_xpath(\"xpath\")]\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "for x in range(len(links)):\n",
    "    links[x].click()\n",
    "    try:\n",
    "        driver.implicitly_wait(3)\n",
    "        DO something\n",
    "        driver.back()\n",
    "        print(\"Mission completed!!\")\n",
    "    except (ElementNotVisibleException, NoSuchElementException):\n",
    "        driver.back()\n",
    "        print(\"No action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(browser.page_source, 'lxml')         # Soup of words\n",
    "words = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "dates = soup.find_all('a',{\"class\" :'TopicPost-timestamp'})\n",
    "\n",
    "print(len(words))\n",
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Code # 1\n",
    "path_to_chromedriver = '../Garage/chromedriver'                          # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)       # Open browser\n",
    "current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page=9'\n",
    "browser.get(current_list)\n",
    "title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "title_list[4].click()\n",
    "browser.find_element_by_class_name('Topic-title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Code # 2\n",
    "path_to_chromedriver = '../Garage/chromedriver'                          # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)       # Open browser\n",
    "browser.get(current_topic + '?page=' + str(next_page))\n",
    "soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "#soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moves down the forum, clicking on each individual title link\n",
    "title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "for i in range(len(title_list)):\n",
    "    print(title_list[i])\n",
    "    title_list[i].click()\n",
    "    sleep(1)\n",
    "    browser.back()\n",
    "    title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "    print(title_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_funct(title_list, list_of_dicts, browser):\n",
    "    for i in range(len(title_list)):\n",
    "        try:\n",
    "            title_list[i].click()                                           # Click on Title i\n",
    "            soup = BeautifulSoup(browser.page_source, 'lxml')               # Soup of all\n",
    "            words = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'}) # Words\n",
    "            dates = soup.find_all('a',{\"class\" :'TopicPost-timestamp'})     # Dates\n",
    "            sleep(0.5)\n",
    "            \n",
    "            for post in range(len(words)):\n",
    "                post_dict = {                                         # Creation & Statement of dicts\n",
    "                    'text' : words[post].text,\n",
    "                    'date' : dates[post].text\n",
    "                }\n",
    "                list_of_dicts.append(post_dict)\n",
    "\n",
    "            browser.back()                                            # Back\n",
    "            title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        except:\n",
    "            print(\"Woops\", forum_page, i)\n",
    "            save_posts(list_of_dicts, saver, forum_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very alpha Psudocode\n",
    "links = driver.find_elements_by_xpath(\"xpath\")\n",
    "links = [link.get_attribute('href') for link in driver.find_elements_by_xpath(\"xpath\")]\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "for x in range(len(links)):\n",
    "    links[x].click()\n",
    "    try:\n",
    "        driver.implicitly_wait(3)\n",
    "        DO something\n",
    "        driver.back()\n",
    "        print(\"Mission completed!!\")\n",
    "    except (ElementNotVisibleException, NoSuchElementException):\n",
    "        driver.back()\n",
    "        print(\"No action\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
