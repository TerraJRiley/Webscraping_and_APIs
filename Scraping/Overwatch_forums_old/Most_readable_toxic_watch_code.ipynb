{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: To scrape as much of the old Overwatch forums as I can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is specifically to show my code from the ToxicWatch project in a way that I hope will be easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pieces of Scraping Code \n",
    " - I used these to test and figure out before either attaching them to the other bits or swapping them out for other chunks of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psudocode\n",
    "links = driver.find_elements_by_xpath(\"xpath\")\n",
    "links = [link.get_attribute('href') for link in driver.find_elements_by_xpath(\"xpath\")]\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "for x in range(len(links)):\n",
    "    links[x].click()\n",
    "    try:\n",
    "        driver.implicitly_wait(3)\n",
    "        DO something\n",
    "        driver.back()\n",
    "        print(\"Mission completed!!\")\n",
    "    except (ElementNotVisibleException, NoSuchElementException):\n",
    "        driver.back()\n",
    "        print(\"No action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Code # 1\n",
    "path_to_chromedriver = '../Garage/chromedriver'                          # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)       # Open browser\n",
    "current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page=9'\n",
    "browser.get(current_list)\n",
    "title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "title_list[4].click()\n",
    "browser.find_element_by_class_name('Topic-title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Code # 2\n",
    "path_to_chromedriver = '../Garage/chromedriver'                          # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)       # Open browser\n",
    "browser.get(current_topic + '?page=' + str(next_page))\n",
    "soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "#soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moves down the forum, clicking on each individual title link\n",
    "title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "for i in range(len(title_list)):\n",
    "    print(title_list[i])\n",
    "    title_list[i].click()\n",
    "    sleep(1)\n",
    "    browser.back()\n",
    "    title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "    print(title_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(browser.page_source, 'lxml')         # Soup of words\n",
    "words = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "dates = soup.find_all('a',{\"class\" :'TopicPost-timestamp'})\n",
    "\n",
    "print(len(words))\n",
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the things that I wanted to get via Selenium created in list form to then be enumerated thought for each post in the Topic.\n",
    "dates      = [e.get_attribute('data-tooltip-content') for e in browser.find_elements_by_class_name('TopicPost-timestamp') if e.text != ' (Edited)']\n",
    "words      = [e.text for e in browser.find_elements_by_class_name('TopicPost-bodyContent')]\n",
    "nums_dict  = [e.get_attribute('data-topic-post') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "post_num   = [e.get_attribute('id') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "auth_posts = [e.text for e in browser.find_elements_by_class_name('Author-posts') if e.get_attribute('data-toggle') == 'tooltip']\n",
    "prof_link  = [e.get_attribute('href') for e in browser.find_elements_by_class_name('Author-avatar ')]\n",
    "all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for use within the scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the getting of the HTML that I'm looking for.\n",
    "def get_html(url):\n",
    "    return BeautifulSoup(requests.get(url).content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to specifically spit out a list of tuples.  \n",
    "# The original purpose of this was to attempt an alternative\n",
    "# to hitting the next page of a Topic until it could not be hit anymore.\n",
    "\n",
    "# tuple[0] is the url of the topic\n",
    "# tuple[1] is the number of pages in the topic\n",
    "\n",
    "def urls_with_numbers(forum_page_url):\n",
    "#current_forum_page = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "    forum_soup = get_html(forum_page_url)                                       # Input URL get Soup\n",
    "    topic_url_list = ['https://us.battle.net' + topic.attrs['href'] for topic in forum_soup.find_all(attrs={'class': \"ForumTopic\"})]\n",
    "    topic_tuples = []                                                           # Above is list of urls of topics in forum page\n",
    "    count = 0                                                                   # Instantiating empty list and starting count\n",
    "    for forum_topic in forum_soup.find_all(attrs={'class': \"ForumTopic\"}):      # for i in all the forum topic infos\n",
    "        posts_num = json.loads(forum_topic.attrs['data-forum-topic'])['lastPosition'] # Turning each info bit into a dict\n",
    "        topic_pages = posts_num//20 + 1                     # Number of posts in Topic / by num allowed per page +1 for 1st\n",
    "        if posts_num%20 != 0 and posts_num > 20:            # If there's a remainder page\n",
    "            topic_pages += 1                                # Add remainder page\n",
    "        topic_tuples.append((topic_url_list[count],topic_pages)) # Add the url to the page amount in tuples\n",
    "        count += 1                                          # Keep track of the count\n",
    "    return topic_tuples                                     # Returns list of tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick function to easily and automatically save what's been scraped up to this point.\n",
    "def save_posts(list_of_dicts, saver, forum_page):\n",
    "    return pd.DataFrame(list_of_dicts, columns = ['text',\n",
    "                            'date',\n",
    "                            'ids_dict',\n",
    "                            'post_num',\n",
    "                            'auth_posts',\n",
    "                            'prof_link',\n",
    "                            'title',\n",
    "                            'forum_page',\n",
    "                            'statuses',\n",
    "                            'topic_url']).to_csv(saver + str(forum_page), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was used to simplify the most recent version of the scraping code.\n",
    "# It'll likely be replaced by the smaller function further down with hardcoded bits removed.\n",
    "words = []\n",
    "dates = []\n",
    "TopicPosts = []\n",
    "title = []\n",
    "forum_page = []\n",
    "prof_link = []\n",
    "auth_posts = []\n",
    "topic_keys  = []\n",
    "def turn_to_dict(post, words = words, \n",
    "                 dates = dates, TopicPosts = TopicPosts, \n",
    "                 title = title, forum_page = forum_page, \n",
    "                 prof_link = prof_link, auth_posts = auth_posts, topic_keys = topic_keys):\n",
    "    post_dict = {\"If you're seeing this\": \"something went wrong.\"}\n",
    "    try:\n",
    "        post_dict = {                               # Creation & Statement of dicts\n",
    "            'text'      : words[post].contents,                     # Text of the post\n",
    "            'date'      : dates[post].attrs['data-tooltip-content'],# Date of the (unedited) post\n",
    "            'ids_dict'  : TopicPosts[post].attrs['data-topic-post'],# Author info & votes of the post\n",
    "            'post_num'  : TopicPosts[post].attrs['id'] ,            # Post number in the topic\n",
    "            'title'     : title,                    # Title of Topic\n",
    "            'forum_page': forum_page,               # Page in the forum\n",
    "            'topic_url' : topic_keys[post][0]}\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        post_dict['statuses']   = TopicPosts[post].attrs['data-topic']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        post_dict['prof_link']  = prof_link[post].attrs['href'][0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        post_dict['auth_posts'] = auth_posts[post][0]  # Number of posts author has made\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return post_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace above function in usability\n",
    "def add_attempt(dictionary, title, addition):\n",
    "    try:\n",
    "        dictionary[str(title)] = addition\n",
    "    except:\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code simplification later.\n",
    "edt = ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for clicking next until the final page of the topic has been scraped.\n",
    "def click_next():\n",
    "    more_pages = True\n",
    "    try:\n",
    "        nexts = []\n",
    "        for element in browser.find_elements_by_class_name('Button-content'):\n",
    "            if element.text == 'NEXT':\n",
    "                nexts.append(element)\n",
    "        nexts[1].click()\n",
    "        #not_last_page = True\n",
    "    except:\n",
    "        more_pages = False\n",
    "    return more_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions of scraping code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the original code that I used to scrape the forum.  I wanted to get a small and simple amount of data and as such this only obtains the first page of every topic and will only grab topics in pages divisible by 5 or 10, depending on what I set it to previously.  Not shown in this is that I used multiple notebooks at the same time to scrape different chunks of the forums.  For example one notebook would be scraping the 1,000 to 5,000 page range while another would scrape the 5,000 to 7,500 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL SCRAPING SAMPLING CODE\n",
    "#ops = webdriver.chrome.options.Options()\n",
    "#ops.add_argument('--dns-prefetch-disable')\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path)#, options=ops)       # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(1,1000):\n",
    "    try:\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        if forum_page % 100 == 0:\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):\n",
    "            try:\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                #print('start dict')\n",
    "                \n",
    "                while more_pages == True:                               # Click on Title i\n",
    "                    soup = BeautifulSoup(browser.page_source, 'lxml')   # Soup of all\n",
    "                    words = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "                    dates = soup.find_all('a',{\"class\" :'TopicPost-timestamp'})\n",
    "                    \n",
    "                    for post in range(len(words)):\n",
    "                        post_dict = {\n",
    "                            'text' : words[post].text,\n",
    "                            'date' : dates[post].text\n",
    "                        }\n",
    "                        list_of_dicts.append(post_dict)\n",
    "         \n",
    "                    more_pages = click_next()                           # Clicks \"NEXT\" otherwise return False\n",
    "                    browser.get(current_list)                           # Return to the current Forum page\n",
    "                    title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "                    \n",
    "            except:                                                     # Troubleshoot if any issues come up\n",
    "                print(\"Woops\", forum_page, i)\n",
    "                save_posts(list_of_dicts, saver, forum_page)\n",
    "                sleep(10)\n",
    "                browser.refresh()\n",
    "                browser.get(current_list)\n",
    "                \n",
    "    except:                                                             # Troubleshoot harder incase of issues\n",
    "        print(\"Woops\", forum_page)\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "        sleep(30)\n",
    "        browser.refresh()\n",
    "        browser.get(current_list)\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an expanded version of the previous code, still with selenium.  I wanted to grab as much information from the forums as possible.\n",
    "\n",
    "I added some sleeping functions to attempt to calm the browser calls.  It did not help.\n",
    "\n",
    "I added the ops to the webdriver because a Stack Overflow page suggested it for the Timeout error I kept getting when running this code.  This also did not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE WITH PROBABLY TOO MANY BROWSER CALLS\n",
    "ops = webdriver.chrome.options.Options()\n",
    "ops.add_argument('--dns-prefetch-disable')\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Pearl_'                          # Path to save\n",
    "\n",
    "for forum_page in range(0,1001):     # BE CAREFUL WITH THIS             # For each page in chunk of posts\n",
    "    try:                                                                # Failsafe within forum page\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)                                       # Go to forum page\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")# Define list of topic pages to click\n",
    "        if forum_page % 100 == 0 & forum_page != 0:                                       # Failsafe saver per 100 pages\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):                                # For every title in the list\n",
    "            try:                                                        # Failsafe in topic page\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                #print('start dict')\n",
    "                #sleep(2)\n",
    "                while more_pages == True:                               # While true that there's more pages...\n",
    "                    title      = browser.find_element_by_class_name('Topic-title').text\n",
    "                    dates      = [e.get_attribute('data-tooltip-content') for e in browser.find_elements_by_class_name('TopicPost-timestamp') if e.text != ' (Edited)']\n",
    "                    words      = [e.text for e in browser.find_elements_by_class_name('TopicPost-bodyContent')]\n",
    "                    nums_dict  = [e.get_attribute('data-topic-post') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "                    #sleep(2)\n",
    "                    post_num   = [e.get_attribute('id') for e in browser.find_elements_by_class_name('TopicPost')]\n",
    "                    auth_posts = [e.text for e in browser.find_elements_by_class_name('Author-posts') if e.get_attribute('data-toggle') == 'tooltip']\n",
    "                    prof_link  = [e.get_attribute('href') for e in browser.find_elements_by_class_name('Author-avatar ')]\n",
    "                    all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "                    auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "                    #print('dict components defined')\n",
    "                    for post in range(len(words)):                      # For each post in topic\n",
    "                        try:\n",
    "                            post_dict = {                               # Creation & Statement of dicts\n",
    "                                'text'      : words[post],              # Text of the post\n",
    "                                'date'      : dates[post],              # Date of the (unedited) post\n",
    "                                'nums_dict' : nums_dict[post],          # Author info & votes of the post\n",
    "                                'post_num'  : post_num[post],           # Post umber in the topic\n",
    "                                'prof_link' : prof_link[post],          # Link to author profile\n",
    "                                'auth_img'  : auth_img[post],           # Profile image of author\n",
    "                                'title'     : title,                    # Title of Topic\n",
    "                                'forum_page': forum_page}               # Page in the forum\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            post_dict['auth_posts'] = auth_posts[post]  # Number of posts author has made\n",
    "                        except:\n",
    "                            pass\n",
    "                        list_of_dicts.append(post_dict)\n",
    "                    more_pages = click_next()                           # Clicks \"NEXT\" otherwise return False\n",
    "                    #print('next clicked')\n",
    "                browser.get(current_list)                               # Returns to current forum page\n",
    "                #sleep(3)                                              # Tiny safety sleep\n",
    "                #print('slept')\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "                \n",
    "            except:\n",
    "                print(\"Woops\", forum_page, i)                           # Informs of the problem\n",
    "                save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "                sleep(10)                                               # Safety sleep\n",
    "                browser.refresh()                                       # Safety Refresh\n",
    "                browser.get(current_list)                               # Return to forum page\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine for freshness\n",
    "    except:\n",
    "        print(\"Woops\", forum_page)                                      # Informs that there's a problem\n",
    "        save_posts(list_of_dicts, saver, forum_page)                    # Save current posts\n",
    "        sleep(30)                                                       # Long Safety Sleep\n",
    "        browser.refresh()                                               # Safety Refresh\n",
    "        browser.get(current_list)                                       # Return to forum page\n",
    "        \n",
    "save_posts(list_of_dicts, saver, forum_page)                            # Finally save the posts\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])             # Return the dataframe for looking! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunitly the timeout error was not easily fixed so I decided to try a simpler approach.  This was when I decided to limit the number of requests that I made for the browser by instead using BeaufitulSoup once I got the code for the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ops = webdriver.chrome.options.Options()\n",
    "#ops.add_argument('--dns-prefetch-disable')\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path)#, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(1,1000):\n",
    "    try:\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        if forum_page % 100 == 0:\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):                                # For every title in the list\n",
    "            try:                                                        # Failsafe in topic page\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                next_page = 2\n",
    "                current_topic = browser.current_url\n",
    "                while more_pages == True:                               # While true that there's more pages...\n",
    "                    soup       = BeautifulSoup(browser.page_source, 'lxml')         # Soup of words\n",
    "                    title      = soup.find(attrs={'class':'Topic-title'}).contents\n",
    "                    TopicPosts = soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "                    dates      = [e for e in soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents != ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']]\n",
    "                    words      = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "                    auth_posts = [e.contents for e in soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "                    prof_link  = soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "                    all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "                    auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "                    for post in range(len(words)):                      # For each post in topic\n",
    "                        try:\n",
    "                            post_dict = {                               # Creation & Statement of dicts\n",
    "                                'text'      : words[post].contents,                     # Text of the post\n",
    "                                'date'      : dates[post].attrs['data-tooltip-content'],# Date of the (unedited) post\n",
    "                                'ids_dict'  : TopicPosts[post].attrs['data-topic-post'],# Author info & votes of the post\n",
    "                                'post_num'  : TopicPosts[post].attrs['id'] ,            # Post number in the topic\n",
    "                                'statuses'  : TopicPosts[post].attrs['data-topic'],\n",
    "                                'prof_link' : prof_link[post].attrs['href'],            # Link to author profile\n",
    "                                'auth_img'  : auth_img[post],           # Profile image of author\n",
    "                                'title'     : title,                    # Title of Topic\n",
    "                                'forum_page': forum_page}               # Page in the forum\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            post_dict['auth_posts'] = auth_posts[post]  # Number of posts author has made\n",
    "                        except:\n",
    "                            pass\n",
    "                        list_of_dicts.append(post_dict)\n",
    "                    try:\n",
    "                        browser.get(current_topic + '?page=' + str(next_page))\n",
    "                        soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "                        if soup.find(attrs = {'class':'error-type'}).contents[0] == '404':\n",
    "                            more_pages = False\n",
    "                        else:\n",
    "                            next_page += 1\n",
    "                    except:\n",
    "                        more_pages = False\n",
    "                browser.get(current_list)                               # Returns to current forum page\n",
    "                #sleep(3)                                                # Tiny safety sleep\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "                \n",
    "            except:\n",
    "                print(\"Woops\", forum_page, i)                           # Informs of the problem\n",
    "                save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "                sleep(10)                                               # Safety sleep\n",
    "                browser.refresh()                                       # Safety Refresh\n",
    "                browser.get(current_list)                               # Return to forum page\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine for freshness\n",
    "    except:\n",
    "        print(\"Woops\", forum_page)\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "        sleep(30)\n",
    "        browser.refresh()\n",
    "        browser.get(current_list)\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it was a nice practice in translating Selenium over to BeautifulSoup, it did not solve the problem.  One might also notice that certain data factors weren't as constantly present, so I needed a try/except statement for those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS is our new official one??\n",
    "path = '../Garage/chromedriver'                                         # Path to Chromedriver\n",
    "browser = webdriver.Chrome(executable_path = path)#, options=ops)         # Open browser\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(72,1000):\n",
    "    try:\n",
    "        current_list = 'https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page)\n",
    "        browser.get(current_list)\n",
    "        title_list = browser.find_elements_by_class_name(\"ForumTopic-title\")\n",
    "        if forum_page % 100 == 0:\n",
    "            save_posts(list_of_dicts, saver, forum_page)\n",
    "\n",
    "        for i in range(len(title_list)):                                # For every title in the list\n",
    "            try:                                                        # Failsafe in topic page\n",
    "                title_list[i].click()                                   # Click on Title i\n",
    "                more_pages = True                                       # True means there's more pages\n",
    "                next_page = 2\n",
    "                current_topic = browser.current_url\n",
    "                while more_pages == True:                               # While true that there's more pages...\n",
    "                    soup       = BeautifulSoup(browser.page_source, 'lxml')         # Soup of words\n",
    "                    title      = soup.find(attrs={'class':'Topic-title'}).contents\n",
    "                    TopicPosts = soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "                    dates      = [e for e in soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents != ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']]\n",
    "                    words      = soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "                    auth_posts = [e.contents for e in soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "                    prof_link  = soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "                    all_imgs   = [e.get_attribute('src') for e in browser.find_elements_by_css_selector('img')]\n",
    "                    auth_img   = [e for e in all_imgs if 'blznav' not in e and len(e) != 0][1:-1]\n",
    "                    for post in range(len(words)):                      # For each post in topic\n",
    "                        try:\n",
    "                            post_dict = {                               # Creation & Statement of dicts\n",
    "                            'text'      : words[post].contents,                     # Text of the post\n",
    "                            'date'      : dates[post].attrs['data-tooltip-content'],# Date of the (unedited) post\n",
    "                            'ids_dict'  : TopicPosts[post].attrs['data-topic-post'],# Author info & votes of the post\n",
    "                            'post_num'  : TopicPosts[post].attrs['id'] ,            # Post number in the topic\n",
    "                            'auth_img'  : auth_img[post],           # Profile image of author\n",
    "                            'title'     : title,                    # Title of Topic\n",
    "                            'forum_page': forum_page}               # Page in the forum\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            post_dict['statuses']   = TopicPosts[post].attrs['data-topic']\n",
    "                            post_dict['prof_link']  = prof_link[post].attrs['href']\n",
    "                            post_dict['auth_posts'] = auth_posts[post]  # Number of posts author has made\n",
    "                        except:\n",
    "                            pass\n",
    "                        list_of_dicts.append(post_dict)\n",
    "                    try:\n",
    "                        browser.get(current_topic + '?page=' + str(next_page))\n",
    "                        soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "                        #print('attempted new page')\n",
    "                        if soup.find(attrs = {'class':'error-type'}).contents[0] == '404':\n",
    "                            #print(\"got a 404\")\n",
    "                            more_pages = False\n",
    "                        #else:\n",
    "                        #    print(\"there is another page and we added 1\")\n",
    "                        #    next_page += 1\n",
    "                    except:\n",
    "                        next_page += 1\n",
    "                browser.get(current_list)                               # Returns to current forum page\n",
    "                #sleep(3)                                                # Tiny safety sleep\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine list for freshness\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Canceled!\")\n",
    "            except:\n",
    "                print(\"TimedOut on Forum Section:\", forum_page,\"Topic number:\", i, \"Topic Page:\", next_page-1)                           # Informs of the problem\n",
    "                save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "                sleep(10)                                               # Safety sleep\n",
    "                browser.refresh()                                       # Safety Refresh\n",
    "                browser.get(current_list)                               # Return to forum page\n",
    "                title_list = browser.find_elements_by_class_name(\"ForumTopic-title\") # Redefine for freshness\n",
    "            #except:\n",
    "            #    print('SOMETHING ELSE WENT WRONG')\n",
    "            #    pass\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Canceled!\")\n",
    "    except:\n",
    "        print(\"TimedOut on Forum Section:\", forum_page)\n",
    "        save_posts(list_of_dicts, saver, forum_page)\n",
    "        sleep(30)\n",
    "        browser.refresh()\n",
    "        browser.get(current_list)\n",
    "    #except:\n",
    "    #    print(\"SOMETHING ELSE WENT WRONG\")\n",
    "    #    pass\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then decided to attempt to find the bug by removing some try/excepts and no dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS is just before deleting a bunch of junk\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_' \n",
    "\n",
    "for forum_page in range(0,100):\n",
    "    #try:\n",
    "    topic_keys = urls_with_numbers('https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page))\n",
    "    if forum_page % 100 == 0:\n",
    "        save_posts(list_of_dicts, saver, forum_page) \n",
    "    for topic_stats in topic_keys:                                # For every title in the list\n",
    "        for page in range(topic_stats[1]):\n",
    "            page += 1               # TRY \"for page +1 in range(topic_stats[1]):\" later\n",
    "            if page > 1:\n",
    "                URL = topic_stats[0] + '?page=' + str(page)\n",
    "            else:\n",
    "                URL = topic_stats[0]\n",
    "            page_soup  = get_html(URL)         # Soup of words\n",
    "            title      = page_soup.find(attrs={'class':'Topic-title'}).contents\n",
    "            dates      = [e for e in page_soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents != ['\\n\\t\\t\\t\\t\\t\\t\\t\\xa0(Edited)\\n']]\n",
    "            words      = page_soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "            TopicPosts = page_soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "            auth_posts = [e.contents for e in page_soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "            prof_link  = page_soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "        for page in range(topic_info[1]):                      # For each post in topic\n",
    "\n",
    "            turn_to_dict(post)\n",
    "            list_of_dicts.append(post_dict)\n",
    "        try:\n",
    "            resoup = get_html(title_list[i] + '?page=' + str(next_page))\n",
    "            if resoup.find(attrs = {'class':'error-type'}).contents[0] == '404':\n",
    "                more_pages = False\n",
    "        except:\n",
    "            next_page += 1\n",
    "        resoup = get_html(current_list)                               # Returns to current forum page\n",
    "        #except KeyboardInterrupt:\n",
    "        #    print(\"Canceled!\")\n",
    "        #except:\n",
    "        #    print(\"TimedOut on Forum Section:\", forum_page,\"Topic number:\", i, \"Topic Page:\", next_page-1)                           # Informs of the problem\n",
    "        #    save_posts(list_of_dicts, saver, forum_page)            # Save current posts\n",
    "        #    sleep(10)    \n",
    "        print(\"Success!:\", forum_page,\"Topic number:\", i, \"Topic Page:\", next_page-1)\n",
    "    #except KeyboardInterrupt:\n",
    "    #    print(\"Canceled!\")\n",
    "    #except:\n",
    "    #    print(\"TimedOut on Forum Section:\", forum_page)\n",
    "    #    save_posts(list_of_dicts, saver, forum_page)\n",
    "    #    sleep(30)\n",
    "    #    resoup = get_html(current_list)\n",
    "save_posts(list_of_dicts, saver, forum_page)\n",
    "df = pd.DataFrame(list_of_dicts, columns = ['text','date'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the latest version of the scraping code.  Very buggy.  At this point I had to shelf the project to focus on other things, both because these edits didn't fix the timeout error and because the effort for my hypothetical fix would have been higher than what I could have given at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS is for turning into the Requests library\n",
    "list_of_dicts = []\n",
    "saver = './data/full_scrapes/Overwatch_Test_'                   # The path & starting name for saving\n",
    "\n",
    "for forum_page in range(101):                                   # Will go through all the forum pages specified\n",
    "    topic_keys = urls_with_numbers('https://us.battle.net/forums/en/overwatch/22813879/?page='+ str(forum_page))\n",
    "    if forum_page % 100 == 0:\n",
    "        save_posts(list_of_dicts, saver, forum_page) \n",
    "    for topic_stats in topic_keys:                              # For every title in the list\n",
    "        for page in range(topic_stats[1]):                      # for page # in range of # of pages in the topic\n",
    "            #page += 1                                          # Add 1 to compensate for starting at 0\n",
    "            if page > 1:                                        # Basically making sure it's not the first page\n",
    "                URL = topic_stats[0] + '?page=' + str(page)     # This is basicslly for getting to all the pages\n",
    "            else:\n",
    "                URL = topic_stats[0]                            # This will be the first page\n",
    "            sleep(5)\n",
    "            \n",
    "            # This chunk defines the lists of things that we want per row\n",
    "            page_soup  = get_html(URL)                          # Soup of the page\n",
    "            title      = page_soup.find(attrs={'class':'Topic-title'}).contents\n",
    "            dates      = [e for e in page_soup.find_all('a',{\"class\" :'TopicPost-timestamp'}) if e.contents !=edt]\n",
    "            words      = page_soup.find_all('div',{\"class\" :'TopicPost-bodyContent'})\n",
    "            TopicPosts = page_soup.find_all(attrs = {\"class\" :'TopicPost'})\n",
    "            auth_posts = [e.contents for e in page_soup.find_all('a',attrs = {'class' :'Author-posts'})]\n",
    "            prof_link  = page_soup.find_all(attrs = {\"class\" :'Author-avatar '})\n",
    "            print(\"F-Page:\", forum_page, \"Posts/Topic:\", topic_stats[1],\"Page:\",page,\"URL:\",URL )\n",
    "            if page == 1:\n",
    "                print(\"Skipped duplicate page\")\n",
    "            else:\n",
    "                for post in range(len(words)):                  # For each post that's present\n",
    "                    list_of_dicts.append(turn_to_dict(post, words, \n",
    "                 dates, TopicPosts, title, forum_page, \n",
    "                 prof_link, auth_posts, topic_keys))            # Add the dict of post to list of posts\n",
    "        print(\"END F-page:\", forum_page,\"Topic Page:\", page, \"Last URL:\", URL) # Sanity check\n",
    "df = save_posts(list_of_dicts, saver, forum_page)               # Should both save the data & create a df to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = save_posts(list_of_dicts, saver, forum_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list_of_dicts, columns = ['text',\n",
    "                            'date',\n",
    "                            'ids_dict',\n",
    "                            'post_num',\n",
    "                            'auth_posts',\n",
    "                            'prof_link',\n",
    "                            'title',\n",
    "                            'forum_page',\n",
    "                            'statuses',\n",
    "                            'topic_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_topic_links = ['https://us.battle.net' + topic.attrs['href'] for topic in resoup.find_all(attrs={'class': \"ForumTopic\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data:\n",
    " - Viewing of the sample data I originally obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_24to77     = pd.read_csv('./data/Overwatch_24to77_posts_77.csv')\n",
    "#df_149        = pd.read_csv('./data/Overwatch_posts_149')          # An error skipped pages 24 to 77\n",
    "df_s0to150    = pd.read_csv('./data/Overwatch_Sample_to1500_160')\n",
    "df_s150to1000 = pd.read_csv('./data/Overwatch_SAMPLE150to1000_990')\n",
    "df_s1500      = pd.read_csv('./data/Overwatch_SAMPLEposts_1500')\n",
    "df_s1500to5k  = pd.read_csv('./data/Overwatch_Sample1500to5k_5000')\n",
    "df_s5kto10k   = pd.read_csv('./data/Overwatch_Sample5kto10k_7250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s0to150['date']      = df_s0to150['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s150to1000['date']   = df_s150to1000['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s1500['date']        = df_s1500['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s1500to5k['date']    = df_s1500to5k['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))\n",
    "df_s5kto10k['date']     = df_s5kto10k['date'].map(lambda x: x.replace('\\t','').replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_listed = [    \n",
    "    df_s0to150,\n",
    "    df_s150to1000,\n",
    "    df_s1500,\n",
    "    df_s1500to5k,\n",
    "    df_s5kto10k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df_s150to1000['text']:\n",
    "    try:\n",
    "        if '<Removed by forum moderator for toxicity>' in text:\n",
    "            print(text)\n",
    "    except:\n",
    "        print(text)\n",
    "        #print(df_s0to150['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs_listed:\n",
    "    for text in df['text']:\n",
    "        try:\n",
    "            if '<Removed' in text:\n",
    "                df.head(1)\n",
    "                print(text)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs_listed:\n",
    "    print(df['date'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps:\n",
    " - Step 0: Before I do anything else I'll have to either fix the bugs in my latest code or revert to a previous version of my scraping strategy.\n",
    " - Once that's taken care of I'd like to create a system for saving what's already been scrapped and which parts have been scrapped.  This way I'll be able to keep track of what needs to be scrapped and where I should start each round of code while also automatically going through and skipping whichever Topic pages have already been scrapped.\n",
    " - Next I'd like to stagger the Forum pages that I'll be grabbing such that each notebook will only pull every odd or even forum page.  This would hopefully be a fix that would most resemble the sample scrapper that I originally used.  This is under the hypothesis that the Timeout Error is based on the sequential requests of grabbing the information.\n",
    " - After this I'd like to attempt either smaller amounts of types of data to grab (ie skipping the image of the individual users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
